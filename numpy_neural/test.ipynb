{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/nmist.ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_mnist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n",
      "File \u001b[1;32mc:\\Users\\esteb\\Documents\\numpy_neural_net\\numpy_neural\\data\\config_mnist.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m dropout_perc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     17\u001b[0m output_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/nmist_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m     19\u001b[0m     nmist \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(input_file)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(nmist\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/nmist.ds'"
     ]
    }
   ],
   "source": [
    "from data.config import *\n",
    "from data.dataset import *\n",
    "from report.dumps import *\n",
    "from nn.model import model\n",
    "from nn.funcs import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "# import data.config_mnist as config\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ds, verbose=False, phase=\"Validation\"):\n",
    "    ds.reset()\n",
    "    hits = 0\n",
    "    mean_loss = 0\n",
    "    while not(ds.iter_done()):\n",
    "        x, y = ds.next()\n",
    "        o, batch_loss = nn.forward(x, y, train=False)\n",
    "        print(o)\n",
    "        hits += batch_hits(o, y)\n",
    "        mean_loss += np.mean(batch_loss)\n",
    "    accuracy = float(hits) / float(ds.size)\n",
    "    mean_loss = float(mean_loss) / float(ds.size)\n",
    "    if verbose:\n",
    "        print(phase + \" Accuracy: \" + str(accuracy) + \" Mean Loss \" + str(mean_loss))\n",
    "    return accuracy, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nn, hp, val_hist, train_hist):\n",
    "    cur_epoch = 1\n",
    "    cur_iter = 1\n",
    "    for i in range(1, hp.epochs+1):\n",
    "        train_loss = 0\n",
    "        hits = 0\n",
    "        cur_trained = 0\n",
    "        while not(hp.ds_train.iter_done()):\n",
    "            x, y = hp.ds_train.next()\n",
    "            o, batch_loss = nn.forward(x, y)\n",
    "            nn.backward(y, o)\n",
    "            nn.update(hp.lr)\n",
    "            hits += batch_hits(o, y)\n",
    "            cur_trained += len(x)\n",
    "            train_loss += np.mean(batch_loss)\n",
    "            if cur_iter % hp.validate_every_no_of_batches == 0:\n",
    "                train_accuracy = float(hits) / float(cur_trained)\n",
    "                train_loss = float(train_loss) / float(cur_trained)\n",
    "                train_hist.add(cur_iter, train_loss, train_accuracy)\n",
    "                val_accuracy, val_loss = test(hp.ds_val, True)\n",
    "                val_hist.add(cur_iter, val_loss, val_accuracy)\n",
    "            cur_iter += 1\n",
    "        cur_epoch += 1\n",
    "        hp.ds_train.reset()\n",
    "    return val_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/nmist.ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load hyperparameters and settings according to dataset enum\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# hp = hyperparams(ConfigEnum.XOR)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#hp = hyperparams(ConfigEnum.IRIS)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfigEnum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#model has number of inputs, number of outputs, and list with sizes of hidden layers\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#requires at least 1 hidden layer, else fails assert\u001b[39;00m\n\u001b[0;32m      8\u001b[0m nn \u001b[38;5;241m=\u001b[39m model(hp\u001b[38;5;241m.\u001b[39minput_size, hp\u001b[38;5;241m.\u001b[39moutput_size, hp\u001b[38;5;241m.\u001b[39mhidden_shapes, tanh, tanh_grad, has_dropout\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mhas_dropout, dropout_perc\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mdropout_perc)\n",
      "File \u001b[1;32mc:\\Users\\esteb\\Documents\\numpy_neural_net\\numpy_neural\\data\\config.py:13\u001b[0m, in \u001b[0;36mhyperparams.__init__\u001b[1;34m(self, config_enum)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_iris\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_mnist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[1;32mc:\\Users\\esteb\\Documents\\numpy_neural_net\\numpy_neural\\data\\config_mnist.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m dropout_perc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     17\u001b[0m output_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/nmist_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m     19\u001b[0m     nmist \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(input_file)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(nmist\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/nmist.ds'"
     ]
    }
   ],
   "source": [
    "#load hyperparameters and settings according to dataset enum\n",
    "# hp = hyperparams(ConfigEnum.XOR)\n",
    "#hp = hyperparams(ConfigEnum.IRIS)\n",
    "hp = hyperparams(ConfigEnum.MNIST)\n",
    "\n",
    "#model has number of inputs, number of outputs, and list with sizes of hidden layers\n",
    "#requires at least 1 hidden layer, else fails assert\n",
    "nn = model(hp.input_size, hp.output_size, hp.hidden_shapes, tanh, tanh_grad, has_dropout=hp.has_dropout, dropout_perc=hp.dropout_perc)\n",
    "\n",
    "val_hist = historian()\n",
    "train_hist = historian()\n",
    "logger = nnlogger(hp.output_log, (\"Epoch\", \"Phase\", \"Iteration\", \"Accuracy\", \"Loss\") )\n",
    "train(nn, hp, val_hist, train_hist)\n",
    "test(hp.ds_test, verbose=True, phase=\"Test\")\n",
    "nnplotter.view(val_hist, train_hist) #see results on plot\n",
    "# logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
